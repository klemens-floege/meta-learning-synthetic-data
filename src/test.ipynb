{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CapturePredictionsCallback(xgb.callback.TrainingCallback):\n",
    "    def __init__(self, dsynthetic):\n",
    "        super().__init__()\n",
    "        self.dsynthetic = dsynthetic\n",
    "        self.predictions = []\n",
    "    \n",
    "    def after_iteration(self, model, epoch, evals_log):\n",
    "        preds = model.predict(self.dsynthetic)\n",
    "        self.predictions.append(preds)\n",
    "        return False  # Return False to continue training, True would stop the training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_learning_dynamics_xgb(D_train, y_train, D_synthetic, num_boost_round=10):\n",
    "    # Convert datasets to DMatrix, a data structure used by XGBoost for efficiency\n",
    "    dtrain = xgb.DMatrix(D_train, label=y_train)\n",
    "    dsynthetic = xgb.DMatrix(D_synthetic)\n",
    "\n",
    "    # Parameters for XGBoost - these can be tuned according to your specific problem\n",
    "    params = {\n",
    "        'objective': 'binary:logistic',\n",
    "        'eval_metric': 'logloss',\n",
    "        'seed': 42\n",
    "    }\n",
    "\n",
    "    # Placeholder for synthetic predictions across boosting rounds\n",
    "    synthetic_predictions = []\n",
    "\n",
    "\n",
    "    # Initialize the custom callback\n",
    "    capture_predictions_cb = CapturePredictionsCallback(dsynthetic)\n",
    "\n",
    "    xgb.train(params, dtrain, num_boost_round=num_boost_round, callbacks=[capture_predictions_cb])\n",
    "\n",
    "    synthetic_predictions = np.array(capture_predictions_cb.predictions) #shape [n_checkpoints, n_samples]\n",
    "\n",
    "\n",
    "    # Compute confidence and uncertainty\n",
    "    confidence = synthetic_predictions.mean(axis=0) #shape [n_samples]\n",
    "    aleatoric_uncertainty = np.mean(synthetic_predictions * (1 - synthetic_predictions), axis=0) #shape [n_samples]\n",
    "\n",
    "    return confidence, aleatoric_uncertainty\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download dataset\n",
    "file_path =  \"../data/Adult_data/adult_dataset.csv\"\n",
    "data = pd.read_csv(file_path, sep=',', nrows=200)\n",
    "# Split the data into features and target variable\n",
    "y_train = data.iloc[:, 0].values  # Target variable is the first column\n",
    "x_train = data.iloc[:, 1:].values  # Features are the rest of the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of synthetic samples: 50\n",
      "Number of filtered synthetic samples: 7\n"
     ]
    }
   ],
   "source": [
    "d_synthetic = x_train[:50]\n",
    "\n",
    "confidence, aleoteric_uncertainty = compute_learning_dynamics_xgb(x_train, y_train, d_synthetic, num_boost_round=20)\n",
    "\n",
    "# Filter synthetic samples based on confidence and uncertainty thresholds\n",
    "confidence_threshold = 0.5\n",
    "uncertainty_threshold = 0.2\n",
    "\n",
    "# Apply thresholds to filter indices\n",
    "filtered_indices = (confidence > confidence_threshold) & (aleoteric_uncertainty < uncertainty_threshold)\n",
    "d_filtered = d_synthetic[filtered_indices]\n",
    "\n",
    "print(f\"Original number of synthetic samples: {d_synthetic.shape[0]}\")\n",
    "print(f\"Number of filtered synthetic samples: {d_filtered.shape[0]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
